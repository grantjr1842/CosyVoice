# CosyVoice Performance Benchmark Report

**Date**: 2026-01-01
**Hardware**: NVIDIA GeForce RTX 2070 (8GB VRAM)

---

## Benchmark Configuration

| Parameter | Value |
|-----------|-------|
| Model | Fun-CosyVoice3-0.5B |
| Text | "The quick brown fox jumps over the lazy dog. Performance optimization is the key to a better user experience." |
| Prompt Audio | interstellar-tars-01-resemble-denoised.wav |
| Iterations | 3 |

### Enabled Optimizations
- ✅ FP16 inference (auto-detected GPU compute capability 7.5)
- ✅ TensorRT for Flow decoder (635 MiB engine)
- ✅ SDPA attention (Flash Attention 2 not available)
- ✅ torch.compile mode='reduce-overhead'

---

## Results

### Average Metrics (3 iterations)

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **First Token Latency (FTL)** | 2.476s | Time to first audio chunk |
| **Real-Time Factor (RTF)** | 3.769 | 3.77x slower than real-time |
| **Throughput** | 0.27 s/s | Audio seconds per wall-clock second |
| **Average Generation Time** | 9.79s | Total time per synthesis |
| **Model Load Time** | 23.43s | One-time startup cost |
| **GPU Memory** | 3.29 GB | Peak VRAM usage |

### Per-Iteration Breakdown

| Iteration | FTL (s) | RTF | Audio Length (s) |
|-----------|---------|-----|------------------|
| 1 | 2.392 | 3.662 | 2.52 |
| 2 | 2.461 | 3.952 | 2.52 |
| 3 | 2.577 | 3.693 | 2.76 |

---

## Analysis

### Performance Summary
- **RTF > 1** means slower than real-time (3.77x slower on RTX 2070)
- **FTL ~2.5s** is acceptable for non-streaming use cases
- **3.29 GB VRAM** leaves headroom on 8GB GPU

### Bottlenecks Identified
1. **LLM Inference**: Qwen2 token generation is the primary bottleneck
2. **Flow Decoder**: TensorRT helps but still significant time
3. **No Flash Attention**: SDPA is slower than Flash Attention 2

### Optimization Opportunities

| Optimization | Expected Impact | Effort |
|--------------|-----------------|--------|
| Flash Attention 2 | ~20% LLM speedup | Medium (requires compatible GPU/driver) |
| 4-bit Quantization (GGUF) | ~2-3x memory reduction, ~1.5x speedup | High |
| Batch processing | Linear throughput scaling | Low |
| Streaming overlapping | Lower perceived latency | Medium |

---

## Comparison Targets

For reference, target performance goals:

| Metric | Current | Target | Gap |
|--------|---------|--------|-----|
| RTF | 3.77 | < 1.0 | 3.77x |
| FTL | 2.5s | < 0.5s | 5x |
| VRAM | 3.29 GB | < 2 GB | 1.6x |

---

## Recommendations

1. **Short-term**: Enable batch processing for multiple requests
2. **Medium-term**: Implement Flash Attention 2 if GPU/driver supports it
3. **Long-term**: Convert to GGUF 4-bit quantization for LLM

---

*Report generated by autonomous agent workflow*
