   Compiling cosyvoice-native-server v0.1.0 (/home/grant/github/CosyVoice-1/rust/native-server)
warning: function `normalize_punctuation` is never used
   --> native-server/src/text_frontend.rs:125:4
    |
125 | fn normalize_punctuation(text: &str) -> String {
    |    ^^^^^^^^^^^^^^^^^^^^^
    |
    = note: `#[warn(dead_code)]` (part of `#[warn(unused)]`) on by default

warning: fields `center` and `device` are never read
   --> native-server/src/utils.rs:123:5
    |
119 | pub struct InverseStftModule {
    |            ----------------- fields in this struct
...
123 |     center: bool,
    |     ^^^^^^
124 |     device: Device,
    |     ^^^^^^

warning: `cosyvoice-native-server` (lib) generated 2 warnings
warning: unused variable: `llm_path`
   --> native-server/src/bin/test_llm_parity.rs:107:9
    |
107 |     let llm_path = model_dir.join("llm.safetensors"); // Or just passing the directory? VarBuilder needs a file or list of files.
    |         ^^^^^^^^ help: if this is intentional, prefix it with an underscore: `_llm_path`
    |
    = note: `#[warn(unused_variables)]` (part of `#[warn(unused)]`) on by default

warning: `cosyvoice-native-server` (bin "test_llm_parity") generated 1 warning (run `cargo fix --bin "test_llm_parity" -p cosyvoice-native-server` to apply 1 suggestion)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 3.49s
     Running `rust/target/debug/test_llm_parity`
Using device: Cuda(CudaDevice(DeviceId(1)))
Loading debug data from "/home/grant/github/CosyVoice-1/rust/native-server/../../debug_llm_data.safetensors"
  input_text_embeds: shape=[1, 7, 896], min=-0.219238, max=0.118286, mean=0.000075
  input_prompt_tokens: shape=[1, 50], min=2.000000, max=98.000000, mean=48.400002
Loading CosyVoice LLM...
  py_sos_emb: shape=[1, 1, 896], min=-1.482765, max=1.230097, mean=0.009289
  py_task_id_emb: shape=[1, 1, 896], min=-1.055770, max=0.917087, mean=-0.009763
  py_prompt_emb: shape=[1, 50, 896], min=-1.529142, max=1.732747, mean=-0.002675
Running debug_forward_one...
LLM: Using speech special token sos=6561 (norm: 790.4866)
LLM: Using speech special token task_id=6563 (norm: 709.7413)
  debug_forward_one: sos_emb mean=0.022045033
  debug_forward_one: text_embeds mean=7.519613e-5
  debug_forward_one: task_id_emb mean=-0.02250772
  debug_forward_one: prompt_emb mean=-0.0056992127
  debug_forward_one: lm_input shape=[1, 59, 896], mean=-0.004828762
  debug_forward_one: y_pred shape=[1, 59, 896], mean=0.018898292
  rust_lm_input: shape=[1, 59, 896], min=-3.656250, max=4.058594, mean=-0.004829
  py_lm_input: shape=[1, 59, 896], min=-1.529142, max=1.732747, mean=-0.002266
  LLM LM Input: max_diff=2.331950, mean_diff=0.368424
  WARNING: Large divergence in LLM LM Input!
  rust_logits: shape=[1, 1, 6761], min=-10.632812, max=9.429688, mean=-1.603182
  py_logits: shape=[1, 1, 6761], min=-12.601562, max=12.234375, mean=-2.150717
  LLM Logits: max_diff=7.628906, mean_diff=1.855576
  WARNING: Large divergence in LLM Logits!
