//! Test Flow parity using freshly generated Python artifacts.
//!
//! Uses `debug_artifacts.safetensors` generated by `generate_fresh_artifacts.py`

use anyhow::Result;
use candle_core::{DType, Device, Tensor};
use candle_nn::VarBuilder;
use cosyvoice_native_server::cosyvoice_flow::{CosyVoiceFlow, CosyVoiceFlowConfig};
use cosyvoice_native_server::flow::FlowConfig;
use std::path::Path;

fn log_stats(name: &str, t: &Tensor) -> Result<()> {
    let t = if t.dtype() != DType::F32 {
        t.to_dtype(DType::F32)?
    } else {
        t.clone()
    };
    let flat = t.flatten_all()?;
    let vec = flat.to_vec1::<f32>()?;
    let mut min = f32::INFINITY;
    let mut max = f32::NEG_INFINITY;
    let mut sum = 0.0f64;
    for &v in vec.iter() {
        if !v.is_nan() && !v.is_infinite() {
            min = f32::min(min, v);
            max = f32::max(max, v);
            sum += v as f64;
        }
    }
    let mean = (sum / vec.len() as f64) as f32;
    println!(
        "  {} stats: min={:.6}, max={:.6}, mean={:.6}, shape={:?}",
        name, min, max, mean, t.shape()
    );
    Ok(())
}

fn compare_tensors(name: &str, rust: &Tensor, python: &Tensor) -> Result<()> {
    let rust = rust.to_dtype(DType::F32)?;
    let python = python.to_dtype(DType::F32)?;

    let diff = rust.sub(&python)?.abs()?;
    let diff_flat = diff.flatten_all()?.to_vec1::<f32>()?;
    let max_diff = diff_flat.iter().cloned().fold(0.0f32, f32::max);
    let mean_diff = diff_flat.iter().sum::<f32>() / diff_flat.len() as f32;

    let rust_flat = rust.flatten_all()?.to_vec1::<f32>()?;
    let py_flat = python.flatten_all()?.to_vec1::<f32>()?;

    println!("\n=== {} Comparison ===", name);
    println!("  Rust first 5: {:?}", &rust_flat[..5.min(rust_flat.len())]);
    println!("  Python first 5: {:?}", &py_flat[..5.min(py_flat.len())]);
    println!("  Max diff: {:.6}, Mean diff: {:.6}", max_diff, mean_diff);

    if max_diff < 1e-3 {
        println!("  ✓ PASS (max_diff < 1e-3)");
    } else if max_diff < 1e-2 {
        println!("  ⚠ WARNING (1e-3 <= max_diff < 1e-2)");
    } else {
        println!("  ✗ FAIL (max_diff >= 1e-2)");
    }

    Ok(())
}

fn main() -> Result<()> {
    println!("=== Test Flow with Fresh Artifacts ===\n");

    let device = Device::new_cuda(0).expect("CUDA device required");
    let repo_root = Path::new(env!("CARGO_MANIFEST_DIR")).join("..").join("..");
    let model_dir = repo_root.join("pretrained_models/Fun-CosyVoice3-0.5B");

    // Load fresh artifacts
    let artifacts_path = repo_root.join("debug_artifacts.safetensors");
    if !artifacts_path.exists() {
        println!(
            "Error: {:?} not found. Run: pixi run python debug_scripts/generate_fresh_artifacts.py",
            artifacts_path
        );
        return Ok(());
    }

    let cpu = Device::Cpu;
    let tensors = candle_core::safetensors::load(&artifacts_path, &cpu)?;

    println!("Loaded artifacts:");
    for (name, t) in tensors.iter() {
        println!("  {}: {:?}", name, t.shape());
    }

    // Extract tensors
    let token = tensors.get("token").expect("token missing");
    let prompt_token = tensors.get("prompt_token").expect("prompt_token missing");
    let prompt_feat = tensors.get("prompt_feat").expect("prompt_feat missing");
    let embedding = tensors.get("embedding").expect("embedding missing");
    let rand_noise = tensors.get("rand_noise").expect("rand_noise missing");
    let python_flow_output = tensors.get("python_flow_output").expect("python_flow_output missing");

    // Convert dtypes
    let token = token.to_dtype(DType::U32)?;
    let prompt_token = prompt_token.to_dtype(DType::U32)?;

    // Transpose prompt_feat from [1, T, 80] to [1, 80, T]
    // Python saves as [B, T, mel_dim], Rust expects [B, mel_dim, T]
    let prompt_feat = prompt_feat.transpose(1, 2)?;

    log_stats("Token", &token)?;
    log_stats("Prompt Token", &prompt_token)?;
    log_stats("Prompt Feat (after transpose)", &prompt_feat)?;
    log_stats("Embedding", &embedding)?;
    log_stats("Rand Noise", &rand_noise)?;
    log_stats("Python Flow Output", &python_flow_output)?;

    // Load Flow model (F32 for precision)
    println!("\n--- Loading Flow Model ---");
    let dtype = DType::F32;
    let flow_path = model_dir.join("flow.safetensors");
    let vb = unsafe { VarBuilder::from_mmaped_safetensors(&[flow_path], dtype, &device)? };

    let flow_cfg = CosyVoiceFlowConfig::default();
    let dit_cfg = FlowConfig::default();
    let flow = CosyVoiceFlow::new(flow_cfg, &dit_cfg, vb)?;
    println!("Flow model loaded.");

    // Move tensors to GPU
    let token = token.to_device(&device)?;
    let prompt_token = prompt_token.to_device(&device)?;
    let prompt_feat = prompt_feat.to_device(&device)?.to_dtype(dtype)?;
    let embedding = embedding.to_device(&device)?.to_dtype(dtype)?;

    // Slice noise to required length (prompt + target mel length)
    // Python prompt_feat was [1, 316, 80] -> after transpose [1, 80, 316]
    // Python token has 104 elements -> target_mel_len = 104 * 2 = 208
    // Total mel len = 316 + 208 = 524
    let prompt_mel_len = prompt_feat.dim(2)?;
    let target_mel_len = token.dim(1)? * 2; // token_mel_ratio = 2
    let total_mel_len = prompt_mel_len + target_mel_len;
    println!("\nMel lengths: prompt={}, target={}, total={}", prompt_mel_len, target_mel_len, total_mel_len);

    // Slice noise: [1, 80, 15000] -> [1, 80, total_mel_len]
    let noise = rand_noise.narrow(2, 0, total_mel_len)?.to_device(&device)?.to_dtype(dtype)?;
    log_stats("Noise (sliced)", &noise)?;

    // Run inference
    println!("\n--- Running Rust Flow Inference ---");
    // Python's CausalMaskedDiffWithDiT.inference uses n_timesteps=1 (see ADR-006)
    let n_timesteps = 1;
    let rust_output = flow.inference(
        &token,
        &prompt_token,
        &prompt_feat,
        &embedding,
        n_timesteps,
        Some(&noise),
    )?;

    log_stats("Rust Flow Output", &rust_output)?;

    // Compare with Python
    let python_flow_output = python_flow_output.to_device(&device)?;
    compare_tensors("Flow Output", &rust_output.to_dtype(DType::F32)?, &python_flow_output)?;

    println!("\n=== Test Complete ===");
    Ok(())
}
